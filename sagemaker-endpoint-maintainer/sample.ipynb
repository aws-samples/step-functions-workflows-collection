{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67841fbf-5318-4363-9737-5c06a0d14a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker ipywidgets --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97baa73b-0744-49b3-a004-eeec3253f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d288130-7f5d-476c-a705-1d5d24b32de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use model_version as 2.* if you're using the open-llama model\n",
    "model_id, model_version, = (\n",
    "    \"huggingface-textgeneration-gpt2\",\n",
    "    \"1.*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561ad2ad-7139-4279-ab45-5f1db8de6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And\n",
    "\n",
    "# Retrieves all Text Generation models available by SageMaker Built-In Algorithms.\n",
    "filter_value = And(\"task == textgeneration\", \"framework == huggingface\")\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=text_generation_models,\n",
    "    value=model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03494b-c3ae-493c-9046-1715d1a7a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "95be4941-e8ec-45df-9380-e1d63779f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id='huggingface-llm-falcon-7b-bf16'\n",
    "model_id='huggingface-textgeneration-open-llama'\n",
    "model_version = '2.0'\n",
    "hub = {}\n",
    "HF_MODEL_ID = \"xlnet-base-cased\"  # Pass any other HF_MODEL_ID from - https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads\n",
    "if model_id == \"huggingface-textgeneration-models\":\n",
    "    hub[\"HF_MODEL_ID\"] = HF_MODEL_ID\n",
    "    hub[\"HF_TASK\"] = \"text-generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e5c07-d94c-4494-ac2f-05426fbb1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.r5.4xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes the pre-trained nvidia-ssd model and parameters.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    env=hub,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    "    tags = [{'Key': 'auto-maintain', 'Value':'true'}]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532cdd5-42e5-4bbc-87ef-ec1d7f581e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model_predictor, text):\n",
    "    \"\"\"Query the model predictor.\"\"\"\n",
    "\n",
    "    encoded_text = text.encode(\"utf-8\")\n",
    "\n",
    "    query_response = model_predictor.predict(\n",
    "        encoded_text,\n",
    "        {\n",
    "            \"ContentType\": \"application/x-text\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the generated text.\"\"\"\n",
    "\n",
    "    model_predictions = json.loads(query_response)\n",
    "    generated_text = model_predictions[\"generated_text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b96a7-7f0b-4c21-88f8-f6b36637b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "# text = \"Which movie won the\"\n",
    "\n",
    "# query_response = query(model_predictor, text)\n",
    "# generated_text = parse_response(query_response)\n",
    "# print(f\"Input text: {text}{newline}\" f\"Generated text: {bold}{generated_text}{unbold}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b075eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for x in range(0, 5):\n",
    "    time.sleep(60)\n",
    "    \n",
    "    newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "    text = \"Which movie won the\"\n",
    "    \n",
    "    query_response = query(model_predictor, text)\n",
    "    generated_text = parse_response(query_response)\n",
    "    print(f\"Input text: {text}{newline}\" f\"Generated text: {bold}{generated_text}{unbold}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81e975-9b42-4752-bdbd-ca60f803d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Clean up the endpoint\n",
    "# Delete the SageMaker endpoint\n",
    "\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
